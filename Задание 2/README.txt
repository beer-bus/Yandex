ЗАДАНИЕ:

Имеется файл с различными оценками асессоров.
Формат файла: login tuid docid jud cjud.
Пояснение к формату: login — логин асессора; uid — id асессора (user id); docid — id оцениваемого документа (document id); jud — оценка асессора (judgement); cjud — правильная оценка (correct judgement); разделитель — табуляция \t.
Оценки могут принимать значение [0, 1], т.е. задание, которое сделали асессоры, имеет бинарную шкалу.
Используя данные об оценках, установите, какие асессоры хуже всего справились с заданием. На какие показатели вы ориентировались и какие метрики вы использовали для ответа на этот вопрос? Можно ли предложить какие-то новые метрики для подсчета качества асессоров с учетом природы оценок у этого бинарного задания?
Опишите подробно все этапы вашего решения.

РЕШЕНИЕ:

1. Проведем первичную оценку и подготовку данных.
Таблица состоит из 250000 строк, пропущенные значения отсутствуют.

Каждая задача встречается по 5 раз (в таблице пара вопросов, в которых эксперты пришли к консенсусу, но противоположному правильному суждению.) - будем считать, что в данных вопросах все асессоры ошиблись

2. Добавим столбец с оценками ответов - если оценка асессора совпадает с правильной оценкой - 1 иначе - 0.

3. Сгруппировав оценки ответов по асессорам с помощью среднего арефметического значения можно оценить в относительных значениях, какие асессоры хуже/лучше всего справились с заданием.
В относительных велечинах хуже всего справились асессоры с id: 56, 3, 118, 390, 234.

В качестве метрики использовалось среднее значение всех ответов для каждого из пользователей.

Можно посмотреть такие метрики, как:
Значение медианы - покажет дал ли пользователь больше правильных или неправильных ответов
Сумма - покажет абсолютное число правильных ответов
Однако данные метрики не покажут полной картины для данной задачи.